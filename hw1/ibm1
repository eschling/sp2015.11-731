#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict

optparser = optparse.OptionParser()
optparser.add_option("-b", "--bitext", dest="bitext", default="data/dev-test-train.de-en", help="Parallel corpus (default data/dev-test-train.de-en)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
optparser.add_option("-i", "--iters", dest="iters", default=5, type="int", help="Number of iterations to run EM")
(opts, _) = optparser.parse_args()

sys.stderr.write("Training with IBM Model 1...")
bitext = [[sentence.strip().split() for sentence in pair.split(' ||| ')] for pair in open(opts.bitext)][:opts.num_sents]
e_vocab = set()
f_e_occur = defaultdict(set)
for (n, (f, e)) in enumerate(bitext):
  for e_j in set(e):
    e_vocab.add(e_j)
    for f_i in set(f):
      f_e_occur[f_i].add(e_j)
  if n % 500 == 0:
    sys.stderr.write(".")

p_e_f = dict()
for f_w in f_e_occur.keys():
  p_e_f[f_w] = defaultdict(lambda: 1.0/len(f_e_occur[f_w]))

for it in range(opts.iters):
  sys.stderr.write('\niter: {}'.format(it+1))
  count = defaultdict(lambda: defaultdict(float))
  for (n, (f,e)) in enumerate(bitext):
    const = 1.0/len(f)
    for i, e_i in enumerate(e):
      norm_e = 0
      for j, f_j in enumerate(f):
        norm_e += p_e_f[f_j][e_i]
      for j, f_j in enumerate(f):
        count[f_j][e_i] += p_e_f[f_j][e_i]/norm_e
    if n % 500 == 0: sys.stderr.write('.')
  for f in count.keys():
    norm = sum(count[f][e] for e in f_e_occur[f])
    for e in f_e_occur[f]:
      p_e_f[f][e] = count[f][e]/float(norm)
del e_vocab
del count
sys.stderr.write('\nComputing alignments based on learned parameters...\n')
for n, (f, e) in enumerate(bitext):
  alignment = []
  for i, e_i in enumerate(e):
    argmax = 0, 0
    for j, f_j in enumerate(f):
      if p_e_f[f_j][e_i] > argmax[1]:
        argmax = j, p_e_f[f_j][e_i]
    alignment.append('{}-{}'.format(argmax[0], i))
  print ' '.join(alignment)

