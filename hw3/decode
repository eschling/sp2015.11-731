#!/usr/bin/env python
import argparse
import sys
import models
import heapq
import grade_util
import pickle
from collections import namedtuple

parser = argparse.ArgumentParser(description='Simple phrase based decoder.')
parser.add_argument('-i', '--input', dest='input', default='data/input', help='File containing sentences to translate (default=data/input)')
parser.add_argument('-t', '--translation-model', dest='tm', default='data/tm', help='File containing translation model (default=data/tm)')
parser.add_argument('-s', '--stack-size', dest='s', default=600, type=int, help='Maximum stack size (default=1)')
parser.add_argument('--start_stack', default=100, type=int)
parser.add_argument('-n', '--num_sentences', dest='num_sents', default=sys.maxint, type=int, help='Number of sentences to decode (default=no limit)')
parser.add_argument('-l', '--language-model', dest='lm', default='data/lm', help='File containing ARPA-format language model (default=data/lm)')
parser.add_argument('--tm_size', default=30, type=int, help='Number of translation model entries to use for each source phrase')
parser.add_argument('--output_hyp', help='output file for winning hypotheses')
parser.add_argument('--rescore', default=50, type=int, help='Number of hypotheses to rescore with marginal per sentence')
parser.add_argument('--reorder_limit', default=50, type=int)
parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False,  help='Verbose mode (default=off)')
parser.add_argument('--insert_phrase', type=int, help='insert new phrases up to k places back in target translation')
opts = parser.parse_args()

tm = models.TM(opts.tm, opts.tm_size)
lm = models.LM(opts.lm)
sys.stderr.write('Decoding %s...\n' % (opts.input,))
input_sents = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

def extract_english_recursive(h):
        return '' if h.predecessor is None else '%s%s ' % (extract_english_recursive(h.predecessor), h.phrase.english)

def get_phrases(h):
  return [] if h.predecessor is None else get_phrases(h.predecessor)+[(h.french, h.phrase)]

def phrase_list_to_english(phrases):
  return ' '.join(p[1][0] for p in phrases)

def phrases_to_english(phrases):
  return ' '.join(p.english for phrase in phrases)

def retranslate(f, e, phrases, logprob):
  better = (logprob, "")
  e_i = 0
  for f_ij, p in phrases:
    len_e = len(p.english.split())
    if len(tm[f_ij]) > 1:
      for new in tm[f_ij]:
        if new.english == p.english: continue
        new_e = e[0:e_i] + new.english.split() + e[e_i+len_e:]
        grade = grade_util.grade(lm, tm, f, tuple(new_e))
        if grade > better[0]: better = (grade, ' '.join(new_e))
    e_i += len_e
  return better

def edit_final(f, h, logprob):
  phrases = get_phrases(h)
  e = h.english.split()
  better = retranslate(f, e, phrases, logprob)
  return better 

def future_cost_table(f):
  def max_tm(source):
    max_t = -sys.maxint - 1
    if source not in tm: return max_t
    for phrase in tm[source]:
      score = phrase.logprob
      lm_state = ()
      for word in phrase.english.split():
        (lm_state, word_logprob) = lm.score(lm_state, word)
        score += word_logprob
      if score > max_t: max_t = score
    return max_t

  cost = [[0 for _ in f] for _ in f]
  for i in range(0, len(f)):
    for j in range(len(f)-i):
      if i==0:
        cost[j][j+i] = max_tm(f[j:j+i+1])
      else:
        max_lp = max_tm(f[j:j+i+1])
        for k in range(j, j+i):
          k_cost = cost[j][k] + cost[k+1][j+i]
          if k_cost > max_lp: max_lp = k_cost
        cost[j][j+i] = max_lp
  return cost

model_score = 0.0
winners = []
hypothesis = namedtuple('hypothesis', 'logprob, lm_state, predecessor, phrase, fcost, english, french, plist')
for ind, f in enumerate(input_sents):
    # The following code implements a DP monotone decoding
    # algorithm (one that doesn't permute the target phrases).
    # Hence all hypotheses in stacks[i] represent translations of 
    # the first i words of the input sentence.
    # HINT: Generalize this so that stacks[i] contains translations
    # of any i words (remember to keep track of which words those
    # are, and to estimate future costs)
    fcost = future_cost_table(f)
    uncovered = tuple(0 for _ in f)
    initial_hypothesis = hypothesis(0.0, lm.begin(), None, None, fcost[0][len(f)-1], "", "", [])

    def decode(size, prev_max):
      stacks = [{} for _ in f] + [{}]
      stacks[0][(initial_hypothesis.lm_state, uncovered)] = initial_hypothesis
      fake_min = [0 for _ in stacks]
      for m, stack in enumerate(stacks[:-1]):
        if m > 0: stacks[m-1] = {}
        # extend the top s hypotheses in the current stack
        for (english, cov), h in heapq.nlargest(opts.s, stack.iteritems(), key=lambda (_, h): h.logprob + h.fcost): # prune
          #sys.stderr.write('{} {}, {}, {}, {} {}\n'.format(cov, h.logprob+h.fcost, h.logprob, h.fcost, h.phrase, english))
          for i in xrange(0, min(len(f), m+opts.reorder_limit)):
            if cov[i] == 1: continue
            for j in xrange(i+1, min(len(f)+1, m+opts.reorder_limit+1)):
                if cov[j-1]==1: break
                english = h.english
                if f[i:j] in tm:
                    covered = [0 for _ in f]
                    start = 0
                    future = 0.0
                    n = 0
                    for k in range(len(f)):
                      if cov[k]==1 or (k >= i and k < j):
                        covered[k] = 1
                        n += 1
                        if start < k: future += fcost[start][k-1]
                        start = k + 1
                    if start < len(f): future += fcost[start][len(f)-1]
                    covered = tuple(covered)
                    for phrase in tm[f[i:j]]:
                      if opts.insert_phrase:
                        orderings = []
                        best_ordering = []
                        best_score = -5000
                        best_lm = tuple()
                        for k in range(max(0, len(h.plist)-opts.insert_phrase), len(h.plist)+1):
                          new_phrases = h.plist[:k] + [(f[i:j], phrase)] + h.plist[k:]
                          lm_state = lm.begin()
                          logprob = 0
                          for p in new_phrases:
                            logprob += p[1][1]
                            for w in p[1][0].split():
                              (lm_state, word_logprob) = lm.score(lm_state, w)
                              logprob += word_logprob
                          logprob += lm.end(lm_state) if j == len(f) else 0.0
                          orderings.append((logprob, new_phrases, lm_state))
                          if logprob > best_score:
                            best_score = logprob
                            best_ordering = new_phrases
                            best_lm = lm_state
                        for rlp, rph, rlm in heapq.nlargest(2, orderings, key=lambda x:x[0]):
                          english = phrase_list_to_english(rph)
                          new_hypothesis = hypothesis(rlp, rlm, h, phrase, future, english, f[i:j], rph)
                          key = (rlm, covered)
                          if key not in stacks[n] or stacks[n][key].logprob < logprob:
                            stacks[n][key] = new_hypothesis
                      else:
                        logprob = h.logprob + phrase.logprob
                        lm_state = h.lm_state
                        new_english = english
                        for word in phrase.english.split():
                            new_english += word + ' '
                            (lm_state, word_logprob) = lm.score(lm_state, word)
                            logprob += word_logprob
                        logprob += lm.end(lm_state) if j == len(f) else 0.0
                        new_hypothesis = hypothesis(logprob, lm_state, h, phrase, future, new_english, f[i:j], h.plist + [(f[i:j], phrase)])
                        key = (lm_state, covered)
                        if key not in stacks[n] or stacks[n][key].logprob < logprob: # second case is recombination
                          if len(stacks[n]) > size:
                            if logprob+future > fake_min[n] and logprob+future > prev_max:
                              stacks[n][key] = new_hypothesis
                          elif len(stacks[n]) <= size and logprob+future > prev_max:
                            if logprob+future < fake_min[n]: fake_min[n] = logprob+future
                            stacks[n][key] = new_hypothesis

      # find best translation by rescoring the best hypotheses
      # using the grading script, which marginalizes over alignments
      if len(stacks[-1]) > 0:
        (win_english, _), winner = max(heapq.nlargest(opts.rescore, stacks[-1].iteritems(), key=lambda (c, h): h.logprob), key=lambda (e,h): grade_util.grade(lm, tm, f, tuple(h.english.strip().split())))
        del stacks
        return win_english, winner
      else:
        del stacks
        return None, None

    stack_size = opts.start_stack
    prev_max_prob = -5000
    win_english, winner = None, None
    win_grade = -5000
    while stack_size <= opts.s:
      eng_0, win_0 = decode(stack_size, prev_max_prob)
      if win_0:
        new_grade = grade_util.grade(lm, tm, f, tuple(win_0.english.strip().split()))
        if new_grade > win_grade:
          win_english = eng_0
          winner = win_0
          prev_max_prob = winner.logprob
          win_grade = new_grade
      stack_size+=100
    win_english = winner.english
    winners.append((winner, win_grade))

    score, edited = edit_final(f, winner, win_grade)
    if score > win_grade:
      print edited
      win_english = edited
      sys.stderr.write("Using edited translation\n")
    else:
      print win_english.strip()

    if opts.verbose:
        def extract_tm_logprob(h):
            return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)
        tm_logprob = extract_tm_logprob(winner)
        model_score += win_grade
        sys.stderr.write('%s LM = %f, TM = %f, Total = %f, %f \n' % 
            (win_english, winner.logprob - tm_logprob, tm_logprob, winner.logprob, win_grade))
if opts.verbose:
  sys.stderr.write('model score: {} \n'.format(model_score))
if opts.output_hyp:
  with open(opts.output_hyp, 'w') as out:
    pickle.dump(winners, out)
